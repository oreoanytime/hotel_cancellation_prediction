{"cells":[{"cell_type":"code","execution_count":null,"id":"00494997","metadata":{"id":"00494997"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","%matplotlib inline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, precision_score, recall_score\n","from sklearn.svm import SVC\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import reciprocal, uniform\n"]},{"cell_type":"code","execution_count":null,"id":"cb179dbd","metadata":{"id":"cb179dbd"},"outputs":[],"source":["df = pd.read_csv('hotel_bookings.csv')"]},{"cell_type":"code","execution_count":null,"id":"87377d99","metadata":{"id":"87377d99"},"outputs":[],"source":["def impute_median(series):\n","    return series.fillna(series.median())"]},{"cell_type":"code","execution_count":null,"id":"2425afc8","metadata":{"id":"2425afc8"},"outputs":[],"source":["#Fill children column with the median of the children\n","df.children = df['children'].transform(impute_median)"]},{"cell_type":"code","execution_count":null,"id":"2bb8a32b-9e6f-492f-9746-09b30f6cdff9","metadata":{"id":"2bb8a32b-9e6f-492f-9746-09b30f6cdff9"},"outputs":[],"source":["# Remove rows with NaN in the 'country' column from the original DataFrame\n","df.dropna(subset=['country'], inplace=True)\n","\n","# Reset the index after removing rows\n","df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"92365c88","metadata":{"id":"92365c88"},"outputs":[],"source":["df['agent_encoded'] = df['agent'].isnull().astype(int)"]},{"cell_type":"code","execution_count":null,"id":"d0fe2638-ed22-46ce-b02d-16b8e40f99fb","metadata":{"id":"d0fe2638-ed22-46ce-b02d-16b8e40f99fb"},"outputs":[],"source":["df['company_encoded'] = df['company'].isnull().astype(int)"]},{"cell_type":"code","execution_count":null,"id":"9578d980-a800-4ec1-864e-805e21f65522","metadata":{"id":"9578d980-a800-4ec1-864e-805e21f65522"},"outputs":[],"source":["# Define the columns you want to select\n","columns_to_select = ['lead_time', 'country', 'deposit_type', 'market_segment', 'assigned_room_type', 'distribution_channel', 'customer_type', 'agent_encoded', 'company_encoded', 'arrival_date_week_number']\n","\n","# Create the new DataFrame by selecting the desired columns\n","final_df = df[columns_to_select]\n"]},{"cell_type":"code","execution_count":null,"id":"69_34e5bqzyD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69_34e5bqzyD","outputId":"4db35f67-dff6-44d2-e6c8-264db6e5bbcb"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Davin\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["(118902, 181)\n","(118902, 184)\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Davin\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n","c:\\Users\\Davin\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["(118902, 192)\n","(118902, 204)\n","(118902, 209)\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\Davin\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n","c:\\Users\\Davin\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n","c:\\Users\\Davin\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["(118902, 213)\n"]}],"source":["from sklearn.preprocessing import OneHotEncoder\n","# Perform one-hot encoding\n","transformed_df = final_df[['lead_time', 'agent_encoded', 'company_encoded']].copy()\n","transformed_df['arrival_date_week_number'] = df['arrival_date_week_number']\n","attributes_to_encode = ['country', 'deposit_type', 'market_segment', 'assigned_room_type', 'distribution_channel', 'customer_type']\n","for attribute in attributes_to_encode:\n","    onehot_encoder = OneHotEncoder(sparse=False)\n","    onehot_encoded = onehot_encoder.fit_transform(final_df[[attribute]])\n","    onehot_encoded_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out([attribute]))\n","\n","    # Concatenate the one-hot encoded features with the original dataframe\n","    transformed_df = pd.concat([transformed_df, onehot_encoded_df], axis=1)\n","    print(transformed_df.shape)\n","\n","scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(transformed_df[['lead_time', 'arrival_date_week_number']])\n","transformed_df[['lead_time', 'arrival_date_week_number']] = scaled_data"]},{"cell_type":"markdown","id":"qhPTrqsQkzRx","metadata":{"id":"qhPTrqsQkzRx"},"source":["# **Split data into Train-Test Sets**"]},{"cell_type":"code","execution_count":null,"id":"Ab-vtDQ8jCFR","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ab-vtDQ8jCFR","outputId":"7b6c213c-e037-488d-b99b-1325350016ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["(118902, 213) (118902,)\n"]}],"source":["X = transformed_df\n","y = df['is_canceled']\n","print(X.shape, y.shape)\n","\n","# Split your data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"]},{"cell_type":"markdown","id":"WHNhfT68iu1T","metadata":{"id":"WHNhfT68iu1T"},"source":["# **Models**"]},{"cell_type":"markdown","id":"vwCdv1r6S9-A","metadata":{"id":"vwCdv1r6S9-A"},"source":["# **6. Support Vector Machine (SVM)**"]},{"cell_type":"markdown","id":"-xPLyDjU70Vu","metadata":{"id":"-xPLyDjU70Vu"},"source":["Build, train and test SVM model.\n","Based on our EDA results, we believe that our attributes have a complex relationship with each other, but we are unsure of what pattern it takes shape. Hence we will be using the Radial Basis Function(RBF) kernal as the baseline."]},{"cell_type":"code","execution_count":null,"id":"h8GbpXMYsdZE","metadata":{"id":"h8GbpXMYsdZE"},"outputs":[],"source":["# Create an SVM model with a radial basis function (RBF) kernel\n","svm_model = SVC(kernel='rbf')\n","svm_model.fit(X_train, y_train)\n","\n","# Make predictions on the test set\n","y_pred = svm_model.predict(X_test)\n","\n","# Calculate performance metrics\n","accuracy = accuracy_score(y_test, y_pred)\n","precision = precision_score(y_test, y_pred)\n","recall = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","\n","# Print the performance metrics\n","print(f'Accuracy: {accuracy}')\n","print(f'Precision: {precision}')\n","print(f'Recall: {recall}')\n","print(f'F1 Score: {f1}\\n')\n","\n","# Print the confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\")\n","print(cm)\n","\n","# Print the classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))\n"]},{"cell_type":"markdown","id":"fc42t7NB7_Tl","metadata":{"id":"fc42t7NB7_Tl"},"source":["Due to the large dataset, 118902 x 213, we forsee that tuning all the parameters simultatenously through RandomizedSearchCV or GridSearchCV may be too computationally intensive and time-consuming. Hence, we will be tuning the parameters manually in stages. We will first proceed to find the best kernal.\n"]},{"cell_type":"code","execution_count":null,"id":"1hkwhKrN7-yH","metadata":{"id":"1hkwhKrN7-yH"},"outputs":[],"source":["kernels = ['linear', 'poly', 'sigmoid']\n","\n","for kernel in kernels:\n","    print(f\"Evaluating Kernel: {kernel}\\n\")\n","    svm_model = SVC(kernel=kernel)\n","    svm_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = svm_model.predict(X_test)\n","\n","    # Calculate performance metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    # Print the performance metrics\n","    print(f'Accuracy: {accuracy}')\n","    print(f'Precision: {precision}')\n","    print(f'Recall: {recall}')\n","    print(f'F1 Score: {f1}\\n')\n","\n","    # Print the confusion matrix\n","    cm = confusion_matrix(y_test, y_pred)\n","    print(\"Confusion Matrix:\")\n","    print(cm)\n","\n","    # Print the classification report\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred))\n","    print(\"\\n\")"]},{"cell_type":"markdown","id":"g-Img5AotZAH","metadata":{"id":"g-Img5AotZAH"},"source":["As we can see, the Linear kernal has the highest precision, but the RBF has the highest f1 score. Thus, we will be futher tuning these 2 kernals.\n","\n","The Linear kernal is primarily only affected by the C parameter, C is the regularization parameter that controls the trade-off between achieving a low training error and a low testing error."]},{"cell_type":"code","execution_count":null,"id":"zbF-Sq0cFwuW","metadata":{"id":"zbF-Sq0cFwuW"},"outputs":[],"source":["# Define a list of C values to test\n","c_values = [0.1, 1, 10, 100]\n","\n","# Iterate over different C values\n","best_precision = 0\n","best_params = {}\n","\n","# Iterate over different C values\n","for c in c_values:\n","    # Create an SVM model with the linear kernel and the current C value\n","    svm_model = SVC(kernel='linear', C=c)\n","    svm_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = svm_model.predict(X_test)\n","\n","    # Calculate performance metrics\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred)\n","    recall = recall_score(y_test, y_pred)\n","    f1 = f1_score(y_test, y_pred)\n","\n","    # Print the performance metrics for the current C value\n","    print(f'Performance metrics for C={c}:')\n","    print(f'Accuracy: {accuracy}')\n","    print(f'Precision: {precision}')\n","    print(f'Recall: {recall}')\n","    print(f'F1 Score: {f1}\\n')\n","\n","    # Track the best precision score and corresponding parameters\n","    if precision > best_precision:\n","        best_precision = precision\n","        best_params = {'C': c}\n","\n","    # Print the confusion matrix\n","    cm = confusion_matrix(y_test, y_pred)\n","    print(\"Confusion Matrix:\")\n","    print(cm)\n","\n","    # Print the classification report\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(y_test, y_pred))\n","    print(\"\\n\")\n","\n","# Print the best parameters found for the best precision score\n","print(\"Best parameters found for the best precision score:\")\n","print(best_params)\n"]},{"cell_type":"markdown","id":"NuychJzFt51q","metadata":{"id":"NuychJzFt51q"},"source":["The RBF kernal is affected by both C and Gamma parameter, Gamma is for non-linear hyperplanes, and defines how far the influence of a single training exmple reached."]},{"cell_type":"code","execution_count":null,"id":"zKzcUAXmFxTS","metadata":{"id":"zKzcUAXmFxTS"},"outputs":[],"source":["# Define lists of C and gamma values to test\n","c_values = [0.1, 1, 10]\n","gamma_values = [0.1, 1, 10, 100]\n","\n","best_precision = 0\n","best_params = {}\n","\n","# Iterate through different C and gamma values\n","for c in c_values:\n","    for gamma in gamma_values:\n","        # Create an SVM model with the RBF kernel and the current C and gamma values\n","        svm_model = SVC(kernel='rbf', C=c, gamma=gamma)\n","        svm_model.fit(X_train, y_train)\n","\n","        # Make predictions on the test set\n","        y_pred = svm_model.predict(X_test)\n","\n","        # Calculate performance metrics\n","        accuracy = accuracy_score(y_test, y_pred)\n","        precision = precision_score(y_test, y_pred)\n","        recall = recall_score(y_test, y_pred)\n","        f1 = f1_score(y_test, y_pred)\n","\n","        # Print the performance metrics for the current C and gamma values\n","        print(f'Performance metrics for C={c} and gamma={gamma}:')\n","        print(f'Accuracy: {accuracy}')\n","        print(f'Precision: {precision}')\n","        print(f'Recall: {recall}')\n","        print(f'F1 Score: {f1}\\n')\n","\n","        # Track the best precision score and corresponding parameters\n","        if precision > best_precision:\n","            best_precision = precision\n","            best_params = {'C': c, 'gamma': gamma}\n","\n","        # Print the confusion matrix\n","        cm = confusion_matrix(y_test, y_pred)\n","        print(\"Confusion Matrix:\")\n","        print(cm)\n","\n","        # Print the classification report\n","        print(\"\\nClassification Report:\")\n","        print(classification_report(y_test, y_pred))\n","        print(\"\\n\")\n","\n","# Print the best parameters found for the best precision score\n","print(\"Best parameters found for the best precision score:\")\n","print(best_params)\n"]},{"cell_type":"markdown","id":"OFAzzhKRuFEm","metadata":{"id":"OFAzzhKRuFEm"},"source":["As we can see, the parameters that provided the best precision score is C = 0.1 and Gamma = 100 with the RBF kernal."]},{"cell_type":"markdown","source":["Our initial range of C and Gamma was 0.1 to 1000 with increments of magnitude 10. However, we saw that the time taken for the model to run took exponentially longer. Also, we saw that the precision score for C = 10 was higher than that of C = 100. We concluded that C = 100 was too large and was already causing overfitting, thus we decided to remove C = 1000 as it was unlikely to return us better results. We had similar results for Gamma as well.\n"],"metadata":{"id":"FZCdMYU0SCCK"},"id":"FZCdMYU0SCCK"},{"cell_type":"markdown","id":"6DUB4WaiugAP","metadata":{"id":"6DUB4WaiugAP"},"source":["Below is the initial standardised approach to hypertuning SVM. But we noted that it was too computationally intensive and time-consuming and we ended up switching to tuning the parameters in 2 stages. Firstly finding the best kernal, followed by the best parameters for those kernals."]},{"cell_type":"code","execution_count":null,"id":"btTOrlnduMs0","metadata":{"id":"btTOrlnduMs0"},"outputs":[],"source":["param_dist = {'C': reciprocal(0.1, 100), 'gamma': reciprocal(0.1, 100),\n","              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n","              'degree': [2, 3, 4],\n","              'coef0': uniform(0, 5)}\n","\n","# Create an SVM model\n","svm_model = SVC()\n","random_search = RandomizedSearchCV(estimator=svm_model, param_distributions=param_dist,\n","                                   n_iter=20, cv=10, scoring='precision', random_state=0, n_jobs=-1, verbose=2)\n","\n","random_search.fit(X, y)\n","\n","print(\"Best parameters found: \", random_search.best_params_)\n","print(\"Best precision score: \", random_search.best_score_)\n"]},{"cell_type":"markdown","metadata":{"id":"jSxrbsYsssqe"},"source":["However, we also do note that although this is better than manually iterating through the parameters in stages, it still may not provide us with the best parameters. This is because the RandomizedSearchCV only tries out a random combination of parameters, and may not necessarily try out the best combination of parameters. Hence, we using GridSearchCV may be a better option, but it will be too computationally intensive and time-consuming. The use of k-fol for corss validation may also help with getting a better model, but this too will add to the computational intensity and time taken."],"id":"jSxrbsYsssqe"},{"cell_type":"markdown","metadata":{"id":"s6vFPpeJssqe"},"source":[],"id":"s6vFPpeJssqe"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}